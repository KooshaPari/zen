services:
  postgres:
    # Use pgvector build to ensure the vector extension is available (works on arm64)
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_PASSWORD: postgres
      POSTGRES_USER: postgres
      POSTGRES_DB: zen
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 10

  arangodb:
    # Newer tag provides multi-arch (arm64) support
    image: arangodb:3.12
    environment:
      ARANGO_ROOT_PASSWORD: root
    ports:
      - "8529:8529"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8529/_api/version"]
      interval: 10s
      timeout: 5s
      retries: 10

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  opensearch:
    image: opensearchproject/opensearch:2.14.0
    # Force amd64 on Apple Silicon; enable Rosetta/QEMU in Docker Desktop if needed
    platform: linux/amd64
    profiles: [search]
    environment:
      - discovery.type=single-node
      - plugins.security.disabled=true
      - OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m
    ports:
      - "9200:9200"
      - "9600:9600"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9200"]
      interval: 10s
      timeout: 5s
      retries: 10

  os_dashboards:
    image: opensearchproject/opensearch-dashboards:2.14.0
    platform: linux/amd64
    profiles: [search]
    environment:
      - OPENSEARCH_HOSTS=["http://opensearch:9200"]
      - DISABLE_SECURITY_DASHBOARDS_PLUGIN=true
    ports:
      - "5601:5601"
    depends_on:
      opensearch:
        condition: service_healthy

  redpanda:
    # Official registry provides multi-arch image
    image: docker.redpanda.com/redpandadata/redpanda:v24.2.1
    command: [
      "redpanda", "start",
      "--overprovisioned", "--smp", "1",
      "--memory", "1G", "--reserve-memory", "0M",
      "--check=false",
      "--kafka-addr", "0.0.0.0:9092",
      "--advertise-kafka-addr", "redpanda:9092"
    ]
    ports:
      - "9092:9092"

  # Ollama (ARM64-native) for local embeddings
  ollama:
    image: ollama/ollama:latest
    environment:
      - OLLAMA_KEEP_ALIVE=24h
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    profiles: [embeddings]

  # Pre-pull a couple of embedding models (optional)
  ollama-init:
    image: curlimages/curl:8.8.0
    depends_on:
      - ollama
    entrypoint: ["sh","-lc"]
    environment:
      - OLLAMA_EMBED_MODEL=${OLLAMA_EMBED_MODEL:-nomic-embed-text}
    command: |
      echo pulling $$OLLAMA_EMBED_MODEL;
      curl -sS -X POST http://ollama:11434/api/pull \
        -H "Content-Type: application/json" \
        -d "{\"name\":\"$$OLLAMA_EMBED_MODEL\"}"
    profiles: [embeddings]

  # Meilisearch (lightweight faceted search)
  meilisearch:
    image: getmeili/meilisearch:v1.9
    environment:
      - MEILI_NO_ANALYTICS=true
      - MEILI_ENV=development
      - MEILI_MASTER_KEY=dev-master-key
    ports:
      - "7700:7700"
    volumes:
      - meili_data:/meili_data
    profiles: [search-lite]

  rabbitmq:
    image: rabbitmq:3-management
    container_name: zen-rabbitmq
    environment:
      RABBITMQ_DEFAULT_USER: guest
      RABBITMQ_DEFAULT_PASS: guest
    ports:
      - "${RABBITMQ_PORT:-5672}:5672"
      - "${RABBITMQ_MGMT_PORT:-15672}:15672"
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "-q", "ping"]
      interval: 10s
      timeout: 5s
      retries: 10

  tei:
    image: ghcr.io/huggingface/text-embeddings-inference:1.5
    # TEI currently publishes amd64; run under emulation on Apple Silicon
    platform: linux/amd64
    profiles: [embeddings]
    environment:
      - MODEL_ID=BAAI/bge-small-en-v1.5
      - MAX_CONCURRENT_REQUESTS=8
    ports:
      - "8082:80"

volumes:
  pgdata:
  ollama_data:
  meili_data:
